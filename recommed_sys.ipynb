{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import textstat\n",
    "import joblib\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422741</th>\n",
       "      <td>i begun to feel distressed for you</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422742</th>\n",
       "      <td>i left feeling annoyed and angry thinking that...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422743</th>\n",
       "      <td>i were to ever get married i d have everything...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422744</th>\n",
       "      <td>i feel reluctant in applying there because i w...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422745</th>\n",
       "      <td>i just wanted to apologize to you because i fe...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422746 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0           i just feel really helpless and heavy hearted   fear\n",
       "1       ive enjoyed being able to slouch about relax a...    sad\n",
       "2       i gave up my internship with the dmrg and am f...   fear\n",
       "3                              i dont know i feel so lost    sad\n",
       "4       i am a kindergarten teacher and i am thoroughl...   fear\n",
       "...                                                   ...    ...\n",
       "422741                 i begun to feel distressed for you   fear\n",
       "422742  i left feeling annoyed and angry thinking that...  anger\n",
       "422743  i were to ever get married i d have everything...    joy\n",
       "422744  i feel reluctant in applying there because i w...   fear\n",
       "422745  i just wanted to apologize to you because i fe...  anger\n",
       "\n",
       "[422746 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_prepare_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset and rename columns.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path to CSV dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Prepared dataframe with columns ['text', 'label'].\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data.rename(columns={'sentence': 'text', 'emotion': 'label'}, inplace=True)\n",
    "    return data\n",
    "\n",
    "data = load_and_prepare_data(r\"D:\\Grad_Proj\\project\\combined_emotion.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 422746 entries, 0 to 422745\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    422746 non-null  object\n",
      " 1   label   422746 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_of_Chars</th>\n",
       "      <th>No_of_Words</th>\n",
       "      <th>No_of_Sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>422746.000000</td>\n",
       "      <td>422746.000000</td>\n",
       "      <td>422746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>97.033980</td>\n",
       "      <td>19.220179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.198156</td>\n",
       "      <td>11.057121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>830.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         No_of_Chars    No_of_Words  No_of_Sents\n",
       "count  422746.000000  422746.000000     422746.0\n",
       "mean       97.033980      19.220179          1.0\n",
       "std        56.198156      11.057121          0.0\n",
       "min         2.000000       1.000000          1.0\n",
       "25%        54.000000      11.000000          1.0\n",
       "50%        86.000000      17.000000          1.0\n",
       "75%       128.000000      25.000000          1.0\n",
       "max       830.000000     178.000000          1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['No_of_Chars'] = data['text'].apply(len)\n",
    "data['No_of_Words'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis= 1).apply(len)\n",
    "data['No_of_Sents'] = data.apply(lambda row: nltk.sent_tokenize(row['text']), axis= 1).apply(len)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df: pd.DataFrame, col: str) -> (pd.DataFrame, LabelEncoder):\n",
    "    \"\"\"\n",
    "    Encode categorical labels to integers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        col (str): Column name of labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dataframe with encoded labels, fitted LabelEncoder.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df, le\n",
    "\n",
    "data, le = encode_labels(data, 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\\nplt.figure(figsize=(12,8))\\nfg = sns.countplot(x= data['label'], palette= cols_color)\\nfg.set_title('count plot of classes')\\nfg.set_xlabel('classes')\\nfg.set_ylabel('count of classes') \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data['label'], palette= cols_color)\n",
    "fg.set_title('count plot of classes')\n",
    "fg.set_xlabel('classes')\n",
    "fg.set_ylabel('count of classes') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" plt.figure(figsize=(12,8))\\nfg = sns.pairplot(data= data, hue= 'label', palette= cols_color)\\nplt.show(fg) \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data= data, hue= 'label', palette= cols_color)\n",
    "plt.show(fg) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    ÿ™ÿ≠ŸàŸäŸÑ POS tag ŸÖŸÜ ÿ¥ŸÉŸÑ NLTK ÿ•ŸÑŸâ ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ŸÖŸÜ WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # ÿßŸÑÿßŸÅÿ™ÿ±ÿßÿ∂Ÿä\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜÿµ ŸÖÿπ ÿ™ÿ≠ÿ≥ŸäŸÜ lemmatization ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ POS tagging.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data['lemmatized_words'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\\nplt.figure(figsize=(12,8))\\nfg = sns.countplot(x= data['label'], palette= cols_color)\\nfg.set_title('count plot of classes')\\nfg.set_xlabel('classes')\\nfg.set_ylabel('count of classes') \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data['label'], palette= cols_color)\n",
    "fg.set_title('count plot of classes')\n",
    "fg.set_xlabel('classes')\n",
    "fg.set_ylabel('count of classes') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' all_words = \" \".join(sentence for sentence in data[\\'lemmatized_words\\'])\\nall_words\\n\\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\\n\\nplt.figure(figsize=(12,8))\\nplt.imshow(wordcloud, interpolation=\\'bilinear\\')\\nplt.axis(\\'off\\')\\nplt.show() '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" all_words = \" \".join(sentence for sentence in data['lemmatized_words'])\n",
    "all_words\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, max_df=0.9, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vectorize_text(df: pd.DataFrame, text_col: str, vectorizer: TfidfVectorizer):\n",
    "    \"\"\"\n",
    "    Vectorize text column using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing text data.\n",
    "        text_col (str): Name of the column with preprocessed text.\n",
    "        vectorizer (TfidfVectorizer): Initialized vectorizer.\n",
    "\n",
    "    Returns:\n",
    "        sparse matrix: TF-IDF features matrix.\n",
    "    \"\"\"\n",
    "    X = vectorizer.fit_transform(df[text_col])\n",
    "    return X\n",
    "\n",
    "sampled_data = data.sample(frac=0.3, random_state=42)\n",
    "X = vectorize_text(data, 'lemmatized_words', tfidf)\n",
    "y = data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def extract_additional_features(text: str) -> list:\\n    words = word_tokenize(text)\\n    num_words = len(words)\\n    num_chars = len(text)\\n    sentiment = sia.polarity_scores(text)['compound']\\n    readability = textstat.flesch_reading_ease(text)\\n    return [num_words, num_chars, sentiment, readability]\\n\\nadditional_features = np.array([extract_additional_features(text) for text in data['lemmatized_words']])\\nscaler = StandardScaler()\\nadditional_features_scaled = scaler.fit_transform(additional_features) \""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def extract_additional_features(text: str) -> list:\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    num_chars = len(text)\n",
    "    sentiment = sia.polarity_scores(text)['compound']\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    return [num_words, num_chars, sentiment, readability]\n",
    "\n",
    "additional_features = np.array([extract_additional_features(text) for text in data['lemmatized_words']])\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled = scaler.fit_transform(additional_features) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from scipy.sparse import hstack\\nfrom scipy.sparse import csr_matrix\\n\\nX_combined = hstack([X, csr_matrix(additional_features_scaled)])\\n '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_combined = hstack([X, csr_matrix(additional_features_scaled)])\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           NN RB JJ NN VBD\n",
       "1         JJ NN JJ JJ NN IN JJ NN JJ NN RB VBP JJ NN RB ...\n",
       "2                                            VB NN NN NN NN\n",
       "3                                              NN VBP NN VB\n",
       "4         VB PRP RB JJ NN VB NN NN NN NN NN NN VBP NN NN...\n",
       "                                ...                        \n",
       "422741                                            VB NNS JJ\n",
       "422742                              VB NN IN JJ NN NN JJ NN\n",
       "422743    RB VB JJ NN NN VBP VB RB VBP RB RB VBP JJ NN N...\n",
       "422744                 NN JJ NN VBP JJ VBP NN VBP JJS CD NN\n",
       "422745                                    JJ NN NN IN NN NN\n",
       "Name: pos_tags, Length: 422746, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pos_tags(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return ' '.join(tag for word, tag in pos_tags)\n",
    "\n",
    "data['pos_tags'] = data['lemmatized_words'].apply(extract_pos_tags)\n",
    "\n",
    "tfidf_pos = TfidfVectorizer()\n",
    "X_pos = tfidf_pos.fit_transform(data['pos_tags'])\n",
    "data['pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [ 47454  39719 114453  27643  96949  11978]\n",
      "After SMOTE: [114453 114453 114453 114453 114453 114453]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(y_train))\n",
    "print(\"After SMOTE:\", np.bincount(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 114453, 4: 96949, 0: 47454, 1: 39719, 3: 27643, 5: 11978})\n",
      "After SMOTE: Counter({4: 114453, 1: 114453, 2: 114453, 0: 114453, 3: 114453, 5: 114453})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 262: 0.1263050337217284\n",
      "index 350: 0.5806697162326915\n",
      "index 478: 0.6752847771992059\n",
      "index 739: 0.4368754843085354\n"
     ]
    }
   ],
   "source": [
    "row = X[0].toarray().flatten()\n",
    "for i, val in enumerate(row):\n",
    "    if val != 0:\n",
    "        print(f'index {i}: {val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7645\n",
      "Test set F1 Score (weighted): 0.7672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75     11863\n",
      "           1       0.71      0.71      0.71      9930\n",
      "           2       0.82      0.79      0.80     28614\n",
      "           3       0.57      0.65      0.61      6911\n",
      "           4       0.86      0.80      0.83     24238\n",
      "           5       0.46      0.55      0.50      2994\n",
      "\n",
      "    accuracy                           0.76     84550\n",
      "   macro avg       0.69      0.72      0.70     84550\n",
      "weighted avg       0.77      0.76      0.77     84550\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.72      0.78      0.75     11863\n",
      "        fear       0.71      0.71      0.71      9930\n",
      "         joy       0.82      0.79      0.80     28614\n",
      "        love       0.57      0.65      0.61      6911\n",
      "         sad       0.86      0.80      0.83     24238\n",
      "     suprise       0.46      0.55      0.50      2994\n",
      "\n",
      "    accuracy                           0.76     84550\n",
      "   macro avg       0.69      0.72      0.70     84550\n",
      "weighted avg       0.77      0.76      0.77     84550\n",
      "\n",
      "F1 Score: 0.7672384289333223\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train Random Forest and evaluate on test set.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Testing features and labels.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Random Forest model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \"\"\"  param_grid = {\n",
    "    'n_estimators': [100, 200, 300],           # ÿπÿØÿØ ÿßŸÑÿ£ÿ¥ÿ¨ÿßÿ±\n",
    "    'max_depth': [None, 10, 20, 30],           # ÿ£ŸÇÿµŸâ ÿπŸÖŸÇ ŸÑŸÑÿ¥ÿ¨ÿ±ÿ©\n",
    "    'min_samples_split': [2, 5, 10]             # ÿ£ŸÇŸÑ ÿπÿØÿØ ÿπŸäŸÜÿßÿ™ ŸÑÿ™ŸÇÿ≥ŸäŸÖ ÿπŸÇÿØÿ©\n",
    "    } \"\"\"\n",
    "    \"\"\"  \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,cv=3, n_jobs=-1, verbose=2, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    print(\"ÿ£ŸÅÿ∂ŸÑ ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™:\", grid_search.best_params_)\n",
    "    print(\"ÿ£ŸÅÿ∂ŸÑ ŸÜÿ™Ÿäÿ¨ÿ© F1 (weighted):\", grid_search.best_score_)\n",
    "    \n",
    "    best_rf_model = grid_search.best_estimator_ \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test set F1 Score (weighted): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    \"\"\" cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    return model\n",
    "\n",
    "random_forest_model = train_and_evaluate(X_train_balanced, y_train_balanced, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison between models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' models = {\\n    \\'Random Forest\\': RandomForestClassifier(),\\n    \\'Naive Bayes\\': MultinomialNB(),\\n    \\'SVM\\': SVC(),\\n    \\'KNN\\': KNeighborsClassifier()\\n}\\n\\nfor name, model in models.items():\\n    model.fit(X_train_balanced, y_train_balanced)\\n    y_pred = model.predict(X_test)\\n    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\\n    print(f\"{name} F1 Score: {f1_score(y_test, y_pred, average=\\'weighted\\'):.2f}\")\\n    print(\"-\" * 30)\\n '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"{name} F1 Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\")\n",
    "    print(\"-\" * 30)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/label_encoder.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(random_forest_model, './models/random_forest_model.pkl')\n",
    "joblib.dump(tfidf, './models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(le, './models/label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(text, model, vectorizer, label_encoder):\n",
    "    processed = preprocess_text(text)\n",
    "    \n",
    "    vect_text = vectorizer.transform([processed])\n",
    "    \n",
    "    prediction = model.predict(vect_text)\n",
    "    return label_encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "print(predict_emotion(\"I feel so happy and joyful today!\", random_forest_model, tfidf, le))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 114453, 4: 96949, 0: 47454, 1: 39719, 3: 27643, 5: 11978})\n",
      "After SMOTE: Counter({4: 114453, 1: 114453, 2: 114453, 0: 114453, 3: 114453, 5: 114453})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2767052</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling, Mary GrandPr√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41865</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130616</th>\n",
       "      <td>Bayou Moon</td>\n",
       "      <td>Ilona Andrews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208324</th>\n",
       "      <td>Means of Ascent</td>\n",
       "      <td>Robert A. Caro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77431</th>\n",
       "      <td>The Mauritius Command</td>\n",
       "      <td>Patrick O'Brian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565083</th>\n",
       "      <td>Cinderella Ate My Daughter: Dispatches from th...</td>\n",
       "      <td>Peggy Orenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8914</th>\n",
       "      <td>The First World War</td>\n",
       "      <td>John Keegan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "book_id                                                      \n",
       "2767052                                   The Hunger Games   \n",
       "3                 Harry Potter and the Philosopher's Stone   \n",
       "41865                                             Twilight   \n",
       "2657                                 To Kill a Mockingbird   \n",
       "4671                                      The Great Gatsby   \n",
       "...                                                    ...   \n",
       "7130616                                         Bayou Moon   \n",
       "208324                                    Means of Ascent    \n",
       "77431                                The Mauritius Command   \n",
       "8565083  Cinderella Ate My Daughter: Dispatches from th...   \n",
       "8914                                   The First World War   \n",
       "\n",
       "                             authors  \n",
       "book_id                               \n",
       "2767052              Suzanne Collins  \n",
       "3        J.K. Rowling, Mary GrandPr√©  \n",
       "41865                Stephenie Meyer  \n",
       "2657                      Harper Lee  \n",
       "4671             F. Scott Fitzgerald  \n",
       "...                              ...  \n",
       "7130616                Ilona Andrews  \n",
       "208324                Robert A. Caro  \n",
       "77431                Patrick O'Brian  \n",
       "8565083              Peggy Orenstein  \n",
       "8914                     John Keegan  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_db = pd.read_csv(\"books.csv\", index_col='book_id')\n",
    "books_db = books_db[['original_title', 'authors']]\n",
    "books_db.rename(columns={'original_title': 'title'}, inplace=True)\n",
    "books_db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 2767052 to 8914\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    9415 non-null   object\n",
      " 1   authors  10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "books_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/not-all-rainbow...</td>\n",
       "      <td>Not All Rainbows and Sunshine: The Darker Side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/ethics-in-ai-po...</td>\n",
       "      <td>Ethics in AI: Potential Root Causes for Biased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/python-tuple-th...</td>\n",
       "      <td>Python Tuple, The Whole Truth and Only the Tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/dates-and-subqu...</td>\n",
       "      <td>Dates and Subqueries in¬†SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://towardsdatascience.com/temporal-differ...</td>\n",
       "      <td>Temporal Differences with Python: First Sample...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>https://medium.com/swlh/brian-chesky-is-an-exa...</td>\n",
       "      <td>Brian Chesky is an Example of What it Means to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>https://medium.com/swlh/5-red-flags-of-online-...</td>\n",
       "      <td>5 Red Flags of Online Business¬†Gurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>https://writingcooperative.com/recognizing-the...</td>\n",
       "      <td>Recognizing These Three Realities Can Help Set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>https://writingcooperative.com/i-remember-it-l...</td>\n",
       "      <td>‚ÄúI Remember It Like It Was Just Yesterday‚Ä¶‚Äù Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>https://writingcooperative.com/how-to-formulat...</td>\n",
       "      <td>How to Formulate a Great Nonfiction Theme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2498 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "id                                                        \n",
       "1     https://towardsdatascience.com/not-all-rainbow...   \n",
       "2     https://towardsdatascience.com/ethics-in-ai-po...   \n",
       "3     https://towardsdatascience.com/python-tuple-th...   \n",
       "4     https://towardsdatascience.com/dates-and-subqu...   \n",
       "5     https://towardsdatascience.com/temporal-differ...   \n",
       "...                                                 ...   \n",
       "2494  https://medium.com/swlh/brian-chesky-is-an-exa...   \n",
       "2495  https://medium.com/swlh/5-red-flags-of-online-...   \n",
       "2496  https://writingcooperative.com/recognizing-the...   \n",
       "2497  https://writingcooperative.com/i-remember-it-l...   \n",
       "2498  https://writingcooperative.com/how-to-formulat...   \n",
       "\n",
       "                                                  title  \n",
       "id                                                       \n",
       "1     Not All Rainbows and Sunshine: The Darker Side...  \n",
       "2     Ethics in AI: Potential Root Causes for Biased...  \n",
       "3     Python Tuple, The Whole Truth and Only the Tru...  \n",
       "4                           Dates and Subqueries in¬†SQL  \n",
       "5     Temporal Differences with Python: First Sample...  \n",
       "...                                                 ...  \n",
       "2494  Brian Chesky is an Example of What it Means to...  \n",
       "2495               5 Red Flags of Online Business¬†Gurus  \n",
       "2496  Recognizing These Three Realities Can Help Set...  \n",
       "2497  ‚ÄúI Remember It Like It Was Just Yesterday‚Ä¶‚Äù Re...  \n",
       "2498          How to Formulate a Great Nonfiction Theme  \n",
       "\n",
       "[2498 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_db = pd.read_csv(\"articles.csv\", index_col='id')  # Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ title, link\n",
    "articles_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2498 entries, 1 to 2498\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     2498 non-null   object\n",
      " 1   title   2498 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 58.5+ KB\n"
     ]
    }
   ],
   "source": [
    "articles_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"./models/random_forest_model.pkl\")\n",
    "vectorizer = joblib.load(\"./models/tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def classify_emotions(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    model: BaseEstimator,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    le: LabelEncoder,\n",
    "    positive_emotions: List[str] = [\"joy\", \"love\", \"surprise\"],\n",
    "    save_path: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ŸäÿµŸÜŸÅ ÿßŸÑŸÖÿ¥ÿßÿπÿ± ŸÅŸä ÿπŸÖŸàÿØ ŸÜÿµŸä ŸàŸäŸèÿ±ÿ¨ÿπ ŸÅŸÇÿ∑ ÿßŸÑÿµŸÅŸàŸÅ ÿ∞ÿßÿ™ ÿßŸÑŸÖÿ¥ÿßÿπÿ± ÿßŸÑÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): ÿØÿßÿ™ÿß ŸÅÿ±ŸäŸÖ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ.\n",
    "        text_column (str): ÿßÿ≥ŸÖ ÿßŸÑÿπŸÖŸàÿØ ÿßŸÑÿ∞Ÿä Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ (ŸÖÿ´ŸÑ: \"title\" ÿ£Ÿà \"book_title\").\n",
    "        model (BaseEstimator): ŸÜŸÖŸàÿ∞ÿ¨ ÿ™ÿµŸÜŸäŸÅ ŸÖÿØÿ±ÿ®.\n",
    "        vectorizer (TfidfVectorizer): ŸÖÿ≠ŸàŸÑ TF-IDF.\n",
    "        scaler (StandardScaler): ŸÖŸÇŸäÿßÿ≥ ÿßŸÑŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ•ÿ∂ÿßŸÅŸäÿ©.\n",
    "        le (LabelEncoder): ŸÖÿ¥ŸÅÿ± ÿßŸÑÿπŸàÿßÿ∑ŸÅ.\n",
    "        positive_emotions (List[str], optional): ŸÇÿßÿ¶ŸÖÿ© ÿ®ÿßŸÑÿπŸàÿßÿ∑ŸÅ ÿßŸÑÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ© ŸÑŸÑÿßÿ≠ÿ™ŸÅÿßÿ∏ ÿ®Ÿáÿß.\n",
    "        save_path (str, optional): ÿ•ÿ∞ÿß ÿ™ŸÖ ÿ™ŸàŸÅŸäÿ±Ÿáÿå ÿ≥Ÿäÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿØÿßÿ™ÿß ŸÅÿ±ŸäŸÖ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸÖÿ≥ÿßÿ± ŸÉŸÖŸÑŸÅ CSV.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: ÿØÿßÿ™ÿß ŸÅÿ±ŸäŸÖ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖÿµŸÜŸÅÿ© ÿ∞ÿßÿ™ ÿßŸÑÿπŸàÿßÿ∑ŸÅ ÿßŸÑÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©.\n",
    "    \"\"\"\n",
    "\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"‚ùå ÿßŸÑÿπŸÖŸàÿØ '{text_column}' ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑÿØÿßÿ™ÿß ŸÅÿ±ŸäŸÖ.\")\n",
    "\n",
    "    df['clean_text'] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    X_text = vectorizer.transform(df['clean_text'])\n",
    "\n",
    "\n",
    "    predicted = model.predict(X_text)\n",
    "    df['emotion'] = le.inverse_transform(predicted)\n",
    "\n",
    "    df_filtered = df[df['emotion'].isin(positive_emotions)]\n",
    "\n",
    "\n",
    "    if save_path:\n",
    "        df_filtered.to_csv(save_path, index=False)\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_clean = classify_emotions(\n",
    "        df=books_db,\n",
    "        text_column=\"title\",\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        le=le,\n",
    "        save_path=\"classified_books.csv\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_clean = classify_emotions(\n",
    "        df=articles_db,\n",
    "        text_column=\"title\",\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        le=le,\n",
    "        save_path=\"classified_articles.csv\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_keywords(df: pd.DataFrame, include_keywords=None, exclude_keywords=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ÿ™ÿ±ÿ¥Ÿäÿ≠ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÉŸÑŸÖÿßÿ™ ŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): ÿ¨ÿØŸàŸÑ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ.\n",
    "        include_keywords (list): ŸÉŸÑŸÖÿßÿ™ Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿ≠ÿ™ŸàŸäŸáÿß ÿßŸÑŸÜÿµ.\n",
    "        exclude_keywords (list): ŸÉŸÑŸÖÿßÿ™ Ÿäÿ¨ÿ® ÿ£ŸÑÿß Ÿäÿ≠ÿ™ŸàŸäŸáÿß ÿßŸÑŸÜÿµ.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ŸÖÿ≠ÿ™ŸàŸâ ÿ®ÿπÿØ ÿßŸÑÿ™ÿµŸÅŸäÿ©.\n",
    "    \"\"\"\n",
    "    if include_keywords:\n",
    "        pattern = '|'.join(include_keywords)\n",
    "        df = df[df['title'].str.contains(pattern, case=False, na=False)]\n",
    "    \n",
    "    if exclude_keywords:\n",
    "        pattern = '|'.join(exclude_keywords)\n",
    "        df = df[~df['title'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_map = {\n",
    "    'sadness': ['joy', 'surprise'],\n",
    "    'anger': ['love', 'joy'],\n",
    "    'fear': ['love', 'joy'],\n",
    "    'joy': ['joy','surprise', 'love'],\n",
    "    'surprise': ['love', 'joy'],\n",
    "    'love': ['joy', 'love', 'surprise']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend_content_filtered(\n",
    "    user_text: str,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    label_encoder,\n",
    "    rec_db: pd.DataFrame,\n",
    "    include_keywords=None,\n",
    "    exclude_keywords=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    #ÿ™ŸàÿµŸäÿ© ÿ®ŸÖÿ≠ÿ™ŸàŸâ ŸÖÿπ ŸÅŸÑÿ™ÿ±ÿ© ÿßÿÆÿ™Ÿäÿßÿ±Ÿäÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©.\n",
    "    \"\"\"\n",
    "    # ÿ™ŸÜÿ∏ŸäŸÅ ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑÿ£ÿπŸÖÿØÿ© ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ÿßŸÑÿ≤ÿßÿ¶ÿØÿ©\n",
    "    rec_db.columns = rec_db.columns.str.strip()\n",
    "\n",
    "    # ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑÿ£ÿπŸÖÿØÿ© ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©\n",
    "    if 'predicted_emotion' not in rec_db.columns or 'original_title' not in rec_db.columns:\n",
    "        raise KeyError(\"‚ö†Ô∏è ÿ™ÿ£ŸÉÿØ ÿ£ŸÜ ÿßŸÑÿØÿßÿ™ÿß ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑÿ£ÿπŸÖÿØÿ©: 'original_title' Ÿà 'predicted_emotion'.\")\n",
    "\n",
    "    # ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿ®ÿßŸÑŸÖÿ¥ÿßÿπÿ± ŸÑŸÑŸÜÿµ\n",
    "    user_emotion = predict_emotion(user_text, model, vectorizer, label_encoder)\n",
    "    print(f\"üîç Detected Emotion: {user_emotion}\")\n",
    "\n",
    "    # ÿ™ÿ≠ÿØŸäÿØ ÿßŸÑŸÖÿ¥ÿßÿπÿ± ÿßŸÑŸÖÿ≥ÿ™ŸáÿØŸÅÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ mood_map\n",
    "    target_emotions = mood_map.get(user_emotion, ['joy', 'calm', 'confidence'])\n",
    "\n",
    "    # ÿ™ÿ±ÿ¥Ÿäÿ≠ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ¥ÿßÿπÿ±\n",
    "    recommended = rec_db[rec_db['predicted_emotion'].isin(target_emotions)]\n",
    "\n",
    "    # ŸÅŸÑÿ™ÿ±ÿ© ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÉŸÑŸÖÿßÿ™\n",
    "    filtered = filter_by_keywords(recommended, include_keywords, exclude_keywords)\n",
    "\n",
    "    # ŸÅŸä ÿ≠ÿßŸÑÿ© ÿπÿØŸÖ Ÿàÿ¨ŸàÿØ ŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿπÿØ ÿßŸÑŸÅŸÑÿ™ÿ±ÿ©\n",
    "    if filtered.empty:\n",
    "        print(\"‚ö†Ô∏è ŸÑÿß ŸäŸàÿ¨ÿØ ŸÖÿ≠ÿ™ŸàŸâ ŸäŸÑÿ®Ÿä ŸÖÿπÿßŸäŸäÿ± ÿßŸÑŸÅŸÑÿ™ÿ±ÿ©. ÿπÿ±ÿ∂ ŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿØŸàŸÜ ŸÅŸÑÿ™ÿ±ÿ©.\")\n",
    "        return recommended[['original_title', 'authors','predicted_emotion']].sample(min(3, len(recommended)))\n",
    "\n",
    "    return filtered[['original_title', 'authors', 'predicted_emotion']].sample(min(3, len(filtered)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(emotion: str, top_n: int = 5) -> List[Dict]:\n",
    "    df = pd.read_csv(\"classified_books.csv\")  \n",
    "\n",
    "    if \"emotion\" not in df.columns:\n",
    "        df[\"emotion\"] = df[\"title\"].apply(predict_emotion)\n",
    "\n",
    "    target_emotions = mood_map.get(emotion, [\"joy\", \"love\", \"surprise\"])\n",
    "\n",
    "    recommended = df[df[\"emotion\"].isin(target_emotions)].head(top_n)\n",
    "\n",
    "    return recommended[[\"title\", \"authors\", \"emotion\"]].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(emotion: str, top_n: int = 5) -> List[Dict]:\n",
    "    df = pd.read_csv(\"classified_articles.csv\")\n",
    "\n",
    "    if \"emotion\" not in df.columns:\n",
    "        df[\"emotion\"] = df[\"title\"].apply(predict_emotion)\n",
    "\n",
    "    target_emotions = mood_map.get(emotion, [\"joy\", \"love\", \"surprise\"])\n",
    "\n",
    "    recommended = df[df[\"emotion\"].isin(target_emotions)].head(top_n)\n",
    "\n",
    "    return recommended[[\"title\", \"url\", \"emotion\"]].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content(emotion: str, top_n: int = 5) -> Dict[str, List[Dict]]:\n",
    "    books = recommend_books(emotion)[:top_n]\n",
    "    articles = recommend_articles(emotion)[:top_n]\n",
    "    return {\n",
    "        \"books\": books,\n",
    "        \"articles\": articles\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books:\n",
      "- Title: The Hunger Games, Authors: Suzanne Collins, Emotion: joy\n",
      "- Title: The Lovely Bones, Authors: Alice Sebold, Emotion: love\n",
      "- Title: Gone Girl, Authors: Gillian Flynn, Emotion: joy\n",
      "- Title: The Time Traveler's Wife, Authors: Audrey Niffenegger, Emotion: joy\n",
      "- Title: A Game of Thrones, Authors: George R.R. Martin, Emotion: joy\n",
      "\n",
      "Articles:\n",
      "- Title: Don‚Äôt Become a Full-Time Content Creator If You Have Low-Risk Tolerance, Link: None, Emotion: joy\n",
      "- Title: <strong class=\"markup--strong markup--h3-strong\">How My MBA Made Me a Better Fiction¬†Writer</strong>, Link: None, Emotion: joy\n",
      "- Title: How to Start Your Novel with Momentum to Finish¬†It, Link: None, Emotion: joy\n",
      "- Title: <strong class=\"markup--strong markup--h3-strong\">Using Propensity-Score Matching to Build Leading Indicators</strong>, Link: None, Emotion: joy\n",
      "- Title: Sparkles aren‚Äôt good¬†UX‚ú®, Link: None, Emotion: joy\n"
     ]
    }
   ],
   "source": [
    "results = recommend_content(emotion=\"sad\")\n",
    "\n",
    "print(\"Books:\")\n",
    "for item in results.get('books', []):\n",
    "    print(f\"- Title: {item.get('title')}, Authors: {item.get('authors')}, Emotion: {item.get('emotion')}\")\n",
    "\n",
    "print(\"\\nArticles:\")\n",
    "for item in results.get('articles', []):\n",
    "    print(f\"- Title: {item.get('title')}, Link: {item.get('link')}, Emotion: {item.get('emotion')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Hunger Games', 'authors': 'Suzanne Collins', 'emotion': 'joy'}\n",
      "{'title': 'The Lovely Bones', 'authors': 'Alice Sebold', 'emotion': 'love'}\n",
      "{'title': 'Gone Girl', 'authors': 'Gillian Flynn', 'emotion': 'joy'}\n",
      "{'title': \"The Time Traveler's Wife\", 'authors': 'Audrey Niffenegger', 'emotion': 'joy'}\n",
      "{'title': 'A Game of Thrones', 'authors': 'George R.R. Martin', 'emotion': 'joy'}\n"
     ]
    }
   ],
   "source": [
    "results = recommend_books(emotion=\"sad\")\n",
    "for book in results:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Don‚Äôt Become a Full-Time Content Creator If You Have Low-Risk Tolerance', 'url': 'https://medium.com/swlh/dont-become-a-full-time-content-creator-if-you-have-low-risk-tolerance-13fa2f77791a', 'emotion': 'joy'}\n",
      "{'title': '<strong class=\"markup--strong markup--h3-strong\">How My MBA Made Me a Better Fiction\\xa0Writer</strong>', 'url': 'https://writingcooperative.com/how-my-mba-made-me-a-better-fiction-writer-d222bc61a5b0', 'emotion': 'joy'}\n",
      "{'title': 'How to Start Your Novel with Momentum to Finish\\xa0It', 'url': 'https://writingcooperative.com/how-to-start-your-novel-with-momentum-to-finish-it-8d001f4908c5', 'emotion': 'joy'}\n",
      "{'title': '<strong class=\"markup--strong markup--h3-strong\">Using Propensity-Score Matching to Build Leading Indicators</strong>', 'url': 'https://towardsdatascience.com/using-propensity-score-matching-to-build-leading-indicators-3e656dccbaf9', 'emotion': 'joy'}\n",
      "{'title': 'Sparkles aren‚Äôt good\\xa0UX‚ú®', 'url': 'https://uxdesign.cc/sparkles-arent-good-ux-4b8199497c68', 'emotion': 'joy'}\n"
     ]
    }
   ],
   "source": [
    "results = recommend_articles(emotion=\"sad\")\n",
    "for article in results:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ ÿßŸÑÿ™ÿßÿ±ŸäÿÆ: 05/07/2025\n",
      "‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÑŸäŸÑ 5 ŸÖŸÜÿ¥Ÿàÿ± ÿßŸÑŸäŸàŸÖ.\n",
      "\n",
      "üìä **ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ**\n",
      "- ÿπÿØÿØ ÿßŸÑŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿßŸÑÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©: 0\n",
      "- ÿπÿØÿØ ÿßŸÑŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÑÿ®Ÿäÿ©: 0\n",
      "- ÿπÿØÿØ ÿßŸÑŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿßŸÑŸÖÿ≠ÿßŸäÿØÿ©: 0\n",
      "\n",
      "\n",
      "‚úÖ ÿßŸÑÿÆÿ∑ÿ© ŸÑŸÑŸäŸàŸÖ ÿßŸÑŸÇÿßÿØŸÖ:\n",
      "- ÿ™ÿ≠ŸÑŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ©.\n",
      "- ÿ™ÿ≠ÿ≥ŸäŸÜ ÿØŸÇÿ© ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨.\n",
      "- ÿπÿ±ÿ∂ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿ¥ŸÉŸÑ ÿ±ÿ≥ŸàŸÖŸä ŸÖÿ®ÿ≥ÿ∑.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "def generate_daily_emotion_report(posts, model, vectorizer, label_encoder):\n",
    "    # ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸäŸàŸÖ\n",
    "    today = datetime.date.today().strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    # ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿ•ŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑ ÿπÿØÿØŸä\n",
    "    X = vectorizer.transform(posts)\n",
    "    \n",
    "    # ÿ™ŸàŸÇÿπÿßÿ™ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ™ŸàŸÇÿπÿßÿ™ ÿ•ŸÑŸâ labels ŸÖŸÅŸáŸàŸÖÿ©\n",
    "    labels = label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    # ÿ≠ÿ≥ÿßÿ® ÿπÿØÿØ ŸÉŸÑ ŸÅÿ¶ÿ©\n",
    "    counter = Counter(labels)\n",
    "    \n",
    "    total_posts = len(posts)\n",
    "    positive_posts = [post for post, label in zip(posts, labels) if label == 'Positive']\n",
    "    negative_posts = [post for post, label in zip(posts, labels) if label == 'Negative']\n",
    "    neutral_posts  = [post for post, label in zip(posts, labels) if label == 'Neutral']\n",
    "    \n",
    "    print(f\"\\nüìÖ ÿßŸÑÿ™ÿßÿ±ŸäÿÆ: {today}\")\n",
    "    print(f\"‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÑŸäŸÑ {total_posts} ŸÖŸÜÿ¥Ÿàÿ± ÿßŸÑŸäŸàŸÖ.\\n\")\n",
    "    \n",
    "    print(\"üìä **ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ**\")\n",
    "    print(f\"- ÿπÿØÿØ ÿßŸÑŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿßŸÑÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©: {counter.get('Positive', 0)}\")\n",
    "    print(f\"- ÿπÿØÿØ ÿßŸÑŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÑÿ®Ÿäÿ©: {counter.get('Negative', 0)}\")\n",
    "    print(f\"- ÿπÿØÿØ ÿßŸÑŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿßŸÑŸÖÿ≠ÿßŸäÿØÿ©: {counter.get('Neutral', 0)}\\n\")\n",
    "    \n",
    "    if positive_posts:\n",
    "        print(\"üå± ÿ£ŸÖÿ´ŸÑÿ© ÿπŸÑŸâ ŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©:\")\n",
    "        for p in positive_posts[:2]:  # ŸÖÿ´ÿßŸÑ: ÿ£ŸàŸÑ 2\n",
    "            print(f\"  - {p}\")\n",
    "    \n",
    "    if negative_posts:\n",
    "        print(\"\\n‚ö†Ô∏è ÿ£ŸÖÿ´ŸÑÿ© ÿπŸÑŸâ ŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ÿ≥ŸÑÿ®Ÿäÿ©:\")\n",
    "        for p in negative_posts[:2]:\n",
    "            print(f\"  - {p}\")\n",
    "    \n",
    "    if neutral_posts:\n",
    "        print(\"\\n‚ÑπÔ∏è ÿ£ŸÖÿ´ŸÑÿ© ÿπŸÑŸâ ŸÖŸÜÿ¥Ÿàÿ±ÿßÿ™ ŸÖÿ≠ÿßŸäÿØÿ©:\")\n",
    "        for p in neutral_posts[:2]:\n",
    "            print(f\"  - {p}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ ÿßŸÑÿÆÿ∑ÿ© ŸÑŸÑŸäŸàŸÖ ÿßŸÑŸÇÿßÿØŸÖ:\")\n",
    "    print(\"- ÿ™ÿ≠ŸÑŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ©.\")\n",
    "    print(\"- ÿ™ÿ≠ÿ≥ŸäŸÜ ÿØŸÇÿ© ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨.\")\n",
    "    print(\"- ÿπÿ±ÿ∂ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿ¥ŸÉŸÑ ÿ±ÿ≥ŸàŸÖŸä ŸÖÿ®ÿ≥ÿ∑.\\n\")\n",
    "\n",
    "# ---------------------\n",
    "# ŸÖÿ´ÿßŸÑ ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ:\n",
    "daily_posts = [\n",
    "    \"I feel really anxious about school tomorrow.\",\n",
    "    \"Had fun with my friends today!\",\n",
    "    \"Why is everything going wrong in my life?\",\n",
    "    \"Watched a peaceful documentary about nature.\",\n",
    "    \"I feel so loved and appreciated.\"\n",
    "]\n",
    "\n",
    "generate_daily_emotion_report(\n",
    "    posts=daily_posts,\n",
    "    model=random_forest_model,\n",
    "    vectorizer=tfidf,\n",
    "    label_encoder=le\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
